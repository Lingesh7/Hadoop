[hduser@master vmware-tools-distrib]$ ifconfig -a
eth1      Link encap:Ethernet  HWaddr 00:50:56:2C:BA:6C  
          inet addr:192.168.1.10  Bcast:192.168.1.255  Mask:255.255.255.0
          inet6 addr: fe80::250:56ff:fe2c:ba6c/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:520 errors:0 dropped:0 overruns:0 frame:0
          TX packets:28 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:31727 (30.9 KiB)  TX bytes:2136 (2.0 KiB)

[hduser@master vmware-tools-distrib]$ cat /etc/sysconfig/network
NETWORKING=yes
HOSTNAME=master
NTPSERVERARGS=iburst
GATEWAY=192.168.222.10
[hduser@master vmware-tools-distrib]$ cat /etc/sysconfig/network-scripts/ifcfg-eth0 
DEVICE=eth1
TYPE=Ethernet
BOOTPROTO=static
IPADDR=192.168.1.10
NETMAASK=255.255.255.0

Mapping the nodes: on all nodes

sudo vi /etc/hosts
192.168.1.11 master
192.168.1.12 dn1
192.168.1.13 dn2
192.168.1.5 base

To set the ipaddress of the nodes (in VM the ipaddress often changing)
sudo ifconfig eth1 192.168.222.129 netmask 255.255.255.0

Set passwordless SSH setup between servers:

ssh-keygen -t rsa 

cat .ssh/id_rsa.pub | ssh hduser@master 'cat >> .ssh/authorized_keys'
ssh hduser@master "chmod 700 .ssh; chmod 640 .ssh/authorized_keys"

cat .ssh/id_rsa.pub | ssh hduser@dn2 'cat >> .ssh/authorized_keys'
ssh hduser@dn2 "chmod 700 .ssh; chmod 640 .ssh/authorized_keys"

cat .ssh/id_rsa.pub | ssh hduser@dn1 'cat >> .ssh/authorized_keys'
ssh hduser@dn1 "chmod 700 .ssh; chmod 640 .ssh/authorized_keys"

sudo groupadd hadoop
sudo usermod -g hadoop hduser

Installing Java:

Download java (JDK -> jdk-8u181-linux-x64.tar.gz) by visiting the following link http://www.oracle.com/technetwork/java/javase/downloads/
place the tar file in /usr/lib/jvm
extract with -> sudo tar -zxf jdk-8u181-linux-x64.tar.gz
cd jdk1.8.0_181

sudo  alternatives --install /usr/bin/java java /usr/lib/jvm/jdk1.8.0_181/bin/java 2
sudo alternatives --config java
java -version

Configure JAVA_HOME in ‘hadoop-env.sh’:

sudo vi /usr/local/hadoop/etc/hadoop/hadoop-env.sh
export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_181

Download and Unpack Hadoop Binaries:
Untar the complete hadoop-2.7.1 package

cd /home/hduser/install/
tar xvzf hadoop-2.7.1.tar.gz
sudo mv hadoop-2.7.1 /usr/local/hadoop
sudo chown -R hduser:hadoop /usr/local/hadoop

sudo ln -s hadoop-2.7.1 hadoop
sudo chown -R hduser:hadoop hadoop


stop firewall:  on both master and slaves
sudo service iptables save
sudo service iptables stop
sudo service iptables status

Update the Configuration Files:

vi .bashrc

# Set Hadoop-related environment variables
export HADOOP_PREFIX=/usr/local/hadoop
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
export HADOOP_MAPRED_HOME=/usr/local/hadoop
export HADOOP_COMMON_HOME=/usr/local/hadoop
export HADOOP_HDFS_HOME=/usr/local/hadoop
export YARN_HOME=/usr/local/hadoop

# Native Path
export HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_PREFIX}/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_PREFIX/lib"

#Java Path
export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_181
export JRE_HOME=/usr/lib/jvm/jdk1.8.0_181/jre
export PATH=$PATH:${JAVA_HOME}/bin:${JRE_HOME}/bin

# Add Hadoop bin/ directory to PATH
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

Create NameNode and DataNode directory:

sudo mkdir -p /usr/local/hadoop_store/tmp
sudo mkdir -p /usr/local/hadoop_store/hdfs/namenode
sudo mkdir -p /usr/local/hadoop_store/hdfs/datanode
sudo mkdir -p /usr/local/hadoop_store/hdfs/secondarynamenode
sudo chown -R hduser:hadoop /usr/local/hadoop_store

sudo chmod -R  777 /usr/local/hadoop_store/

Configure the Default File system:

sudo vi /usr/local/hadoop/etc/hadoop/core-site.xml (Master+Slave nodes)

<configuration>
<property>
<name>hadoop.tmp.dir</name>
<value>/usr/local/hadoop_store/tmp</value>
<description>A base for other temporary directories.</description>
</property>
<property>
<name>fs.default.name</name>
<value>hdfs://master:54310</value>
<description>
The name of the default file system. A URI whose scheme and authority determine the FileSystem
implementation. The uris scheme determines the config property fs.SCHEME.impl) naming the
FileSystem implementation class. The uris authority is used to determine the host, port, etc. for a
filesystem.
</description>
</property>
</configuration>

Configure the HDFS::

sudo vi /usr/local/hadoop/etc/hadoop/hdfs-site.xml (Namenode on Master and datanode setting only on slave machines)

<configuration>
<property>
<name>dfs.replication</name>
<value>2</value>
<description>Default block replication.
The actual number of replications can be specified when the file is created.
The default is used if replication is not specified in create time.
</description>
</property>
<property>
<name>dfs.namenode.name.dir</name>
<value>file:/usr/local/hadoop_store/hdfs/namenode</value>
</property>
<property>
<name>dfs.datanode.data.dir</name>
<value>file:/usr/local/hadoop_store/hdfs/datanode</value>
</property>
<property>
<name>dfs.namenode.checkpoint.dir</name>
<value>file:/usr/local/hadoop_store/hdfs/secondarynamenode</value>
</property>
<property>
<name>dfs.namenode.checkpoint.period</name>
<value>3600</value>
</property>
</configuration>

Configure YARN framework:

sudo vi /usr/local/hadoop/etc/hadoop/yarn-site.xml (All Master and Slave nodes)

<configuration>
<!-- Site specific YARN configuration properties -->
<property>
 <name>yarn.nodemanager.aux-services</name>
 <value>mapreduce_shuffle</value>
</property>
<property>
<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
<value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>
<property>
 <name>yarn.resourcemanager.scheduler.address</name>
 <value>master:8030</value>
</property> 
<property>
 <name>yarn.resourcemanager.address</name>
 <value>master:8032</value>
</property>
<property>
  <name>yarn.resourcemanager.webapp.address</name>
  <value>master:8088</value>
</property>
<property>
  <name>yarn.resourcemanager.resource-tracker.address</name>
  <value>master:8031</value>
</property>
<property>
  <name>yarn.resourcemanager.admin.address</name>
  <value>master:8033</value>
</property>

Configure MapReduce framework:

cp /usr/local/hadoop/etc/hadoop/mapred-site.xml.template /usr/local/hadoop/etc/hadoop/mapred-site.xml

sudo vi /usr/local/hadoop/etc/hadoop/mapred-site.xml

<configuration>
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>
</configuration>

<property>
  <name>mapreduce.jobhistory.address</name>
  <value>192.168.1.11:10020</value>
  <description>Host and port for Job History Server (default 0.0.0.0:10020)</description>
</property>

Run only in master
------------------
sudo vi /usr/local/hadoop/etc/hadoop/slaves (edit on both master and slaves)
dn1
dn2
sudo vi /usr/local/hadoop/etc/hadoop/masters (edit only in master)
master

The first step in starting up your Hadoop installation is formatting the Hadoop file-system:
(At master)

hdfs namenode -format


start-all.sh
OR
To start the Daemons separately HDFS and YARN (Useful when hdfs and yarn daemons installed separately)
start-yarn.sh (Resource Manager and Node manager)
start-dfs.sh (namenode, datanode and secondarynamenode)
OR
To start the Daemons individually ( Useful in multinode cluster setup)
hadoop-daemons.sh start secondarynamenode
hadoop-daemons.sh start namenode
hadoop-daemons.sh start datanode
yarn-daemon.sh start nodemanager
yarn-daemon.sh start resourcemanager
mr-jobhistory-daemon.sh start historyserver

==============================================================================================
`HDFS`

'-getmerge' command in Hadoop is for merging files existing in the HDFS file system into a single file in the local file system.
	hdfs dfs –getmerge [-nl] <src> <localdest>

'-stat' command will return filenames only 
	hdfs dfs -stat "%n" my/path/a*		<-- without "%n" shows only the date and time of the file creation 

'list all the file names match with a word'
	hdfs dfs -ls /testing/project/CVDP/CVDPDEV/Workflow/properties/java/vendor* | awk '{print $8}' | while read f; do hdfs dfs -cat $f | grep SFTP && echo $f; done
		--> connectType=SFTP
		--> /testing/project/CVDP/CVDPDEV/Workflow/properties/java/vendorEMSimBlngData.properties
		--> Picked up JAVA_TOOL_OPTIONS: -Xmx1024m
		--> connectType=SFTP
		--> /testing/project/CVDP/CVDPDEV/Workflow/properties/java/vendorFordTrialReports.properties
		--> Picked up JAVA_TOOL_OPTIONS: -Xmx1024m
		--> Picked up JAVA_TOOL_OPTIONS: -Xmx1024m
		--> connectType=SFTP
		--> /testing/project/CVDP/CVDPDEV/Workflow/properties/java/vendorIntrepidWivi.properties
		--> Picked up JAVA_TOOL_OPTIONS: -Xmx1024m
		--> connectType=SFTP

'list the namenode and datanodes of a cluster from any node'
	>hdfs dfsadmin -report
	>hdfs getconf -confKey fs.defaultFS
	 --> hdfs://hdd2cluster
	>hdfs getconf -namenodes
	-->hpchdd2.hpc.ford.com hpchdd2x.hpc.ford.com
	> hdfs getconf -secondaryNamenodes
	-->Incorrect configuration: secondary namenode address dfs.namenode.secondary.http-address is not configured.


'to list the total size of the hdfs cluster'
	> hadoop fs -df hdfs:/
		Picked up JAVA_TOOL_OPTIONS: -Xmx1024m
		Filesystem                      Size              Used        Available  Use%
		hdfs://hdd2cluster  2231764506501120  1876979249534162  343133050934542   84%

	>  hadoop fs -df -h
		Picked up JAVA_TOOL_OPTIONS: -Xmx1024m
		Filesystem           Size   Used  Available  Use%
		hdfs://hdd2cluster  2.0 P  1.8 P    182.8 T   91%

	> hdfs dfsadmin -report
		Picked up JAVA_TOOL_OPTIONS: -Xmx1024m
		Configured Capacity: 2231764506501120 (1.98 PB)
		Present Capacity: 2229593096735457 (1.98 PB)
		DFS Remaining: 201308280757920 (183.09 TB)
		DFS Used: 2028284815977537 (1.80 PB)
		DFS Used%: 90.97%
		Replicated Blocks:
		        Under replicated blocks: 0
		        Blocks with corrupt replicas: 0
		        Missing blocks: 0
		        Missing blocks (with replication factor 1): 0
		        Low redundancy blocks with highest priority to recover: 0
		        Pending deletion blocks: 138
		Erasure Coded Block Groups:
		        Low redundancy block groups: 0
		        Block groups with corrupt internal blocks: 0
		        Missing block groups: 0
		        Low redundancy blocks with highest priority to recover: 0
		        Pending deletion blocks: 0

'Changing the replication factor of a file:'
	hadoop fs -setrep -w 2 <filename>

'Check whether namenode in safemode and leave safe mode:'
	hdfs dfsadmin -safemode get
	hdfs dfsadmin -safemode leave

'Fix corrupt HDFS Files:'
		hdfs fsck /  (or "hdfs fsck / | egrep -v '^\.+$' | grep -v eplica")
	This will list the corrupt HDFS blocks:
		hdfs fsck -list-corruptfileblocks
	Once you find a file that is corrupt
		hdfs fsck /path/to/corrupt/file -locations -blocks -files
		hdfs fs -rm /path/to/file/with/permanently/missing/blocks
	(or)
		bin/hadoop fsck / -delete

'To list the property/parameter value set on hdfs:'
	hdfs getconf -confKey fs.defaultFS

'To Investigate on particular application run on yarn providing jobhistory server started:'
	Open the YARN UI, and inspect the dashboard of recent jobs for that ID 
	(OR)
  	yarn application -status <application_1496499143480_0003>
 	yarn logs -applicationId <application_1496499143480_0003> | more

'to get the quota of a directory for HDFS'
	> hdfs dfs -count -q -h -v  /user/cvdpdevp
	
	       QUOTA       REM_QUOTA     SPACE_QUOTA REM_SPACE_QUOTA    DIR_COUNT   FILE_COUNT       CONTENT_SIZE PATHNAME
	        none             inf           750 G         749.0 G          272          614            350.8 M /user/cvdpdevp

'Find how many columns/fields using IFS - available in a CSV file'
	
	awk -F ',' '{print NR,NF,$0}' file_test.txt <-- prints line number, count of columns, entire line ‘print $0’ means print the current line) 
	awk -F ',' 'NF != 31' file_test.txt <-- prints entire column matching the condition (in place od NF we can also specify $1 or $7 etc)

	Reference for basi awk <--- https://www.gnu.org/software/gawk/manual/html_node/Very-Simple.html

'Below is the query to get the path and file_count for each path. Query has to be update with respect to the source.'

	INSERT OVERWRITE DIRECTORY 'hdfs://hdp2cluster/tmp/spedipin/CitySolutions/'
	ROW FORMAT DELIMITED
	FIELDS TERMINATED BY '\t'
	STORED AS TEXTFILE
	SELECT reverse(substr(reverse(path),instr(reverse(path),'/'),length(path))) as files_path,count(1) as number_of_files 
	FROM fsimage_txt_delimited_partitioned  
	WHERE date_partition='20200327' and path like '%CitySolutions%' and fsize < 31457280 group by reverse(substr(reverse(path),instr(reverse(path),'/'),length(path)));

================================================================
`MapReduce`

The maximum number of attempts to run a task is controlled by the mapreduce.map.maxattempts property for map tasks and mapreduce.reduce.maxattempts for reduce tasks



================================================================

1. Discover the exact DB2 Version (and FixPack Level):
https://www-01.ibm.com/support/docview.wss?uid=swg21642926

2. Download the proper IBM DB2 JDBC Driver for the DB2 version/FP level gathered on Step #1
https://www-01.ibm.com/support/docview.wss?uid=swg27007053

3. Determine the port number of DB2 database instance:
https://www-01.ibm.com/support/docview.wss?uid=swg21343520

4. Copy the downloaded DB2 JDBC JAR file to Sqoop-client library directory:
/usr/hdp/current/sqoop-client/lib

5. Test connection with the following Sqoop syntax (example provided using default port number and doing a list database command):
sqoop list-databases --connect jdbc:db2://<FQDN of DB2 Host>:50000/SAMPLE --username <userid> --password <password>
$sqoop import --driver com.ibm.db2.jcc.DB2Driver --connect jdbc:db2://master:50000 --username mling --password Sanjeevin@1 --table db2tbl --split-by tbl_primarykey --target-dir sqoopimports

This article created by Hortonworks Support (Article: 000004491) on 2016-06-23 12:25
OS: Linux
Type: Configuration
Version: 2.3.0, 2.3.4, 2.4.0

===========================================================================
cd ~/install
tar xvzf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz
sudo mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha /usr/local/sqoop

export SQOOP_HOME=/usr/local/sqoop
export PATH=$PATH:$SQOOP_HOME/bin

cd $SQOOP_HOME/conf
mv sqoop-env-template.sh sqoop-env.sh
echo 'export HADOOP_COMMON_HOME=/usr/local/hadoop' >> $SQOOP_HOME/conf/sqoop-env.sh
echo 'export HADOOP_MAPRED_HOME=/usr/local/hadoop' >> $SQOOP_HOME/conf/sqoop-env.sh

sqoop list-databases --connect jdbc:db2://base:50000/SAMPLE --username mling --password Sanjeevin@1
sqoop list-tables --connect jdbc:db2://base:50000/SAMPLE --username mling -P

sqoop import --connect jdbc:db2://base:50000/SAMPLE --username MLING -P -table EMPLOYEE -m 1 --delete-target-dir;

sqoop export --connect jdbc:db2://base:50000/SAMPLE --username MLING -P -table EMP_SQ --export-dir /user/hduser/EMPLOYEE/part-m-00000  --fields-terminated-by ',' --lines-terminated-by '\n';

sqoop import --connect jdbc:db2://base:50000/SAMPLE --username MLING -P -table EMPLOYEE -m 3 --split-by empno --target-dir EMP_m3 --delete-target-dir;

sqoop import --connect jdbc:db2://base:50000/SAMPLE --username MLING -P -table EMPLOYEE -m 1 --where "job != 'MANAGER'" --append --target-dir EMP_m3 ;

sqoop import  --connect jdbc:db2://192.168.1.6:50000/SAMPLE --username MLING --password Sanjeevin@1 -table EMP_PHOTO -m 1 --target-dir EMP_PHOTO --delete-target-dir --as-avrodatafile;

sqoop export --connect jdbc:db2://192.168.1.6:50000/SAMPLE --username MLING --password Sanjeevin@1 -table EMP_PHOTO_HDFS -m 1 --export-dir /user/hduser/EMP_PHOTO/ --map-column-java "PICTURE=String" --fields-terminated-by ',' --lines-terminated-by '\n';

sqoop export --connect jdbc:db2://192.168.1.6:50000/SAMPLE --username MLING --password Sanjeevin@1 -table EMP_PHOTO_HDFS -m 1 --export-dir /user/hduser/EMP_PHOTO/ ;

https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.0/bk_dataintegration/content/ch_using-sqoop.html

sqoop import-all-tables --connect jdbc:db2://base:50000/SAMPLE --username mling --password Sanjeevin@1 --warehouse-dir '/user/hduser/alltables' -m 1

sqoop eval --connect jdbc:db2://base:50000/SAMPLE --username mling --password Sanjeevin@1 --query "select * from emp_sq where sex='M'" 

--delete-dir and --apend are mutually exclusive

--split-by need to be used when there is --num-mappers (by def 4) is more than 1 and  no primary key is available on the target table 
the column used in splitby should be numeric(varchar also possible but with enabling special flag -Dorg.apache.sqoop.splitter.allow_text_filter=true), or sequence or incremented values. (high no of diif values)
or enable --autoreset-to-one-mapper -- this helps when there is no primary key on the target table it resets the num mappers to one. 



sqoop export \
--connect jdbc:mysql://base:3306/empoffice?zeroDateTimeBehavior=CONVERT_TO_NULL \
--username hive \
--password hive \
--export-dir /user/hduser/external/book_m/yr=1996/000000_0 \
--table books \
--lines-terminated-by '\n' \
--fields-terminated-by '~';


sqoop import \
--connect jdbc:mysql://base:3306/empoffice?zeroDateTimeBehavior=CONVERT_TO_NULL \
--username hive \
--password hive \
--target-dir /user/hduser/external/books_q \
--table books \
--lines-terminated-by '\n' \
--fields-terminated-by '~' \
--as-avrodatafile \
--num-mappers 1 ;



************Errors and Solution*****************
Command used: sqoop import --connect jdbc:mysql://base:3306/custdb?zeroDateTimeBehavior=CONVERT_TO_NULL --username hive --password hive --table credits_cst --delete-target-dir --target-dir /user/hduser/credits_cst/ -m 1

ERROR sqoop.Sqoop: Got exception running Sqoop: java.lang.R    untimeException: java.lang.RuntimeException: java.sql.SQLException: The conne    ction property 'zeroDateTimeBehavior' acceptable values are: 'CONVERT_TO_NULL    ', 'EXCEPTION' or 'ROUND'. The value 'convertToNull' is not acceptable.
java.lang.RuntimeException: java.lang.RuntimeException: java.sql.SQLException    : The connection property 'zeroDateTimeBehavior' acceptable values are: 'CONV    ERT_TO_NULL', 'EXCEPTION' or 'ROUND'. The value 'convertToNull' is not accept    able.

Work Around: (https://stackoverflow.com/questions/48436347/sqoop-import-error)
			sqoop import --connect jdbc:mysql://base:3306/custdb?zeroDateTimeBehavior=CONVERT_TO_NULL --username hive --password hive --table credits_cst --delete-target-dir --target-dir /user/hduser/credits_cst/ -m 1

===========================================================================================
`Hive:`
=======
Refer https://data-flair.training/blogs/apache-hive-metastore/ --> for hive set up
	  https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin#AdminManualMetastoreAdmin-Local/EmbeddedMetastoreDatabase(Derby)

cd /home/hduser/install/
tar xvzf apache-hive-0.14.0-bin.tar.gz
sudo mv apache-hive-0.14.0-bin /usr/local/hive

hadoop fs -mkdir -p /user/hive/warehouse/
hadoop fs -chmod g+w /user/hive/warehouse
hadoop fs -chmod g+w /tmp

echo 'export HADOOP_HOME=/usr/local/hadoop' >> /usr/local/hive/bin/hive-config.sh

cp /usr/local/hive/conf/hive-env.sh.template /usr/local/hive/conf/hive-env.sh
echo 'export HADOOP_HOME=/usr/local/hadoop' >> /usr/local/hive/conf/hive-env.sh

export HADOOP_HOME=/home/hduser/hadoop-2.6.5
export HIVE_HOME=/usr/local/hive
export PATH=$PATH:$HIVE_HOME/bin
export CLASSPATH=$CLASSPATH:/usr/local/hadoop/lib/*:.
export CLASSPATH=$CLASSPATH:/usr/local/hive/lib/*:.
*/
Install mysql - place mysql jdbc connector and place in /usr/local/hive/lib

cd /usr/local/hive/conf
mv hive-default.xml.template hive-site.xml

Make below changes in hive-site.xml

sudo vi /usr/local/hive/conf/hive-site.xml
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?> 
<configuration>
<!-- -
<property> 
	<name>hive.metastore.uris</name> 
	<value>thrift://192.168.1.7:9083</value>
	<description>IP address (or fully-qualified domain name) and port of the metastore host</description>
</property> 
<!- -->
<property>
	<name>javax.jdo.option.ConnectionURL</name>
	<value>jdbc:mysql://192.168.1.7:3306/metastore?createDatabaseIfNotExist=true&amp;useSSL=false</value>
	<description>the URL of the MySQL database</description>
</property>
<property>
	<name>javax.jdo.option.ConnectionDriverName</name>
	<value>com.mysql.cj.jdbc.Driver</value>
</property>
<property>
    <name>hive.metastore.warehouse.dir</name>
    <value>/user/hive/warehouse</value>
    <description>location of default database for the warehouse</description>
 </property>
<property>
	<name>javax.jdo.option.ConnectionUserName</name>
	<value>hive</value>
</property>
<property>
	<name>javax.jdo.option.ConnectionPassword</name>
	<value>hive</value>
</property>
<!-- -
<property>
	<name>hive.stats.dbclass</name>
	<value>jdbc:mysql</value>
<description>The default database that stores temporary hive statistics.</description>
</property>
<property>
	<name>hive.stats.jdbcdriver</name>
	<value>com.mysql.cj.jdbc.Driver</value>
<description>The JDBC driver for the database that stores temporary hive statistics.</description> </property>
<property>
	<name>hive.stats.dbconnectionstring</name>
	<value>jdbc:mysql://192.168.1.7:3306/metastore</value>
	<description>The defaultconnection string for the database that stores temporary hive statistics.</description>
</property>

<property>
	<name>datanucleus.autoCreateSchema</name>
	<value>false</value>
</property>
<property>
 	<name>datanucleus.fixedDatastore</name>
 	<value>true</value>
</property>
<property>
	<name>datanucleus.autoCreateTables</name>
	<value>True</value>
 </property>
 <property>
   <name>datanucleus.autoStartMechanism</name>
   <value>SchemaTable</value>
 </property>
<!- -->

"before starting the hive session - Run in mysql session"
	create database metastore;
	USE metastore;
	SOURCE /usr/local/hive/scripts/metastore/upgrade/mysql/hive-schema-0.14.0.mysql.sql;  <<---- Creates the schema for hive metastore
	show tables;
	CREATE USER 'hiveuser'@'%' IDENTIFIED BY 'hivepassword';
	GRANT all on *.* to 'hiveuser'@localhost identified by 'hivepassword';
	flush privileges;
	quit;

	or 
	create database metastore; in mysql
	USE metastore;
	CREATE USER 'hiveuser'@'ipaddress' IDENTIFIED BY 'hivepassword';
	GRANT all on *.* to 'hiveuser'@localhost identified by 'hivepassword';
	flush privileges;
	$HIVE_HOME/bin/schematool -initSchema -dbType mysql (run in new shell)   <<---- Creates the schema for hive metastore  

hive --service metastore

hive -hiveconf hive.root.logger=DEBUG,console

set hive.cli.print.header=true; 

==================================================================
`HIVE DDL`
==========
'Creat database'
	CREATE DATABASE BOOKSDB;
	USE BOOKSDB;

"Sample file"
Header Info:  "ISBN";"BookTitle";"BookAuthor";"Year";"Publisher";"ImageURLS";"ImageURLM";"ImageURLL"
Value : 	  "0195153448";"Classical Mythology";"Mark P. O. Morford";"2002";"Oxford University Press";"http://images.amazon.com/images/P/0195153448.01.THUMBZZZ.jpg";"http://images.amazon.com/images/P/0195153448.01.MZZZZZZZ.jpg";"http://images.amazon.com/images/P/0195153448.01.LZZZZZZZ.jpg"

Default Delimiters:
 Each fields are delimited by CTRL-A (ie Octal \001).
 If a field is a complex type(such as ARRAY, STRUCT or MAP) containing Primitive Type(like INT, STRING), then each value inside the complex type is delimited by CTRL-B (\002)
 In a map, Key & Value is always delimited by ^C)

'Create normal table'
	CREATE TABLE BOOKS(ISBN INT,BOOKTITLE STRING,BOOKAUTHOR STRING,YEAR INT,PUBLISHER STRING,IMAGEURLS STRING,IMAGEURLM STRING,IMAGEURLL STRING) 
	ROW FORMAT DELIMITED 
	FIELDS TERMINATED BY ','
	STORED AS TEXTFILE;

	LOAD DATA LOCAL INPATH "/home/hduser/BX/BX-BooksCorrected.txt" OVERWRITE INTO TABLE BOOKS;

'Rename' the table itself
	ALTER TABLE BOOKS RENAME TO BOOKLIST;

'Change datatype' of column isbn from INT to STRING: (we can even change the column name too)
	ALTER TABLE BOOKS CHANGE ISBN ISBN_ STRING; 

'Change delimiter'
	ALTER TABLE BOOKS SET SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' WITH SERDEPROPERTIES ('field.delim' = '\;');

'Drop Columns':  (we can also change the order of columns)
	ALTER TABLE BOOKS REPLACE COLUMNS (ISBN STRING, BOOKTITLE STRING, BOOKAUTHOR STRING, YEAR INT, PUBLISHER STRING); 

'NO_DROP' - Alter the table enable/disable protection to NO_DROP, which prevents a table from being dropped, or OFFLINE, which prevents data (not metadata) in a table from being queried:
		ALTER TABLE c_employee ENABLE NO_DROP;
		ALTER TABLE c_employee DISABLE NO_DROP;

	We can stop a partition form being queried by using the ENABLE OFFLINE clause with ALTER TABLE statement.
		ALTER TABLE c_employee ENABLE OFFLINE;
		ALTER TABLE c_employee DISABLE OFFLINE;

'DROPing Tables'
	1. Drop external table (drops data & schema) 
		ALTER TABLE table_name SET TBLPROPERTIES ('EXTERNAL'='FALSE');
		DROP TABLE table_name;

	2. Drop external table (keep data & remove only the structure frm hive)
		DROP TABLE external_hive_table;

'Indexing in Hive'
	Whenever we perform a query on a table that has an index, there is no need for the query to scan all the rows in the table. Further, it checks the index first and then goes to the particular column and performs the operation.
	An Index acts as a reference to the records

Types - 'COMPACT Index'- stores the pair of indexed column’s value and its blockid. 
	  - 'BITMAP Index' - stores the combination of indexed column value and list of rows as a bitmap

	'Creating Index'
		CREATE INDEX index_name
		ON TABLE table_name (columns,....)
		AS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'  |  'BITMAP'  <-- refers the type of idex to be created
		WITH DEFERRED REBUILD;

		ALTER INDEX index_nam on table_name REBUILD;

	We can have any number of indexes for a particular table and any type of indexes as well.
	On the same column we can create both type of indexes on same column.

	"SHOW FORMATTED INDEX ON MOVIES;"  <-- list the no of indexes on the tables if exists
	--result
	idx_name                tab_name                col_names               idx_tab_name            idx_type                comment
	
	mid_index               movies                  mid                     movies__movies_mid_index__      compact
	bit_indx                movies                  mid                     movies__movies_bit_indx__       bitmap
	--
	'Drop Index'
		DROP INDEX IF EXISTS BIT_INDX ON MOVIES;
	
	'When not to use index in hive:'
		Indexes are advised to build on the columns on which you frequently perform operations.
		Building more number of indexes also degrade the performance of your query.
		Type of index to be created should be identified prior to its creation (if your data requires bitmap you should not create compact).This leads to increase in time for executing your query.

'Statistics'
	
	Issue the command "describe formatted table_name;"
	Table Parameters: <-- look for parameters and the below variable shoul set to true if atas are available for this table
	    COLUMN_STATS_ACCURATE   true <-- shows the table stats are collected
	To update the column statistics:
		"ANALYZE TABLE MOVIES COMPUTE STATISTICS FOR COLUMNS MID,MNAME;"

	DESC FORMATTED MOVIES MID; <-- to fetch column stats info
	# col_name              data_type               min                     max                     num_nulls               distinct_count          avg_col_len                max_col_len             num_trues               num_falses              comment
	mid                     int                     1                       193886 	                 0                       96484                                                                                                                      from deserializer

# https://cwiki.apache.org/confluence/display/Hive/StatsDev


==============
`FILE FORMATS`

'AVRO file format' 
"LOAD to HIVE EXTERNAL TABLE FROM ARVO FILE"
	CREATE EXTERNAL TABLE BOOKS_AV(ISBN INT,BOOKTITLE STRING,BOOKAUTHOR STRING,YEAR INT,PUBLISHER STRING)
	ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
	STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
	OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
	LOCATION '/user/hduser/external/books_av';

 If we extracted avro schema then no need to specify column structure. Command to extract avro schema from .avro format file is below
		java -jar $SCOOP_HOME/lib/avro-tools-1.8.1.jar getschema hdfs:///user/hduser/external/books_q/part-m-00000.avro > /tmp/books_av.avsc
	
	CREATE EXTERNAL TABLE books_avs
	STORED AS AVRO
	LOCATION '/user/hduser/external/books_av/'
	TBLPROPERTIES ('avro.schema.url'='/tmp/books_av.avsc');	<-- schema /path/file 

'PARQUET file format'
	Parquet stores binary data in a column-oriented way, where the values of each column are organized so that they are all adjacent, enabling better compression

	CREATE EXTERNAL TABLE BOOKS_PQ(ISBN STRING,BOOKTITLE STRING,BOOKAUTHOR STRING,YEAR INT,PUBLISHER STRING)
	STORED AS PARQUET 
	LOCATION '/user/hduser/external/books_pq';
	
	We cannot load text file directly into parquet table,
	we should first create an staging table to store the text file and use insert overwrite command to write the data in parquet format.

'ORC file format'
	Supports ACID Transactions & Cost-based Optimizer (CBO). Stores column-level metadata.


===============
`Partitioning`

'Static Partitioning'
	set hive.exec.dynamic.partition=true
	set hive.exec.dynamic.partition.mode=strict
	set hive.mapred.mode=strict; <--the strict mode ensures that the queries on partitioned tables cannot execute without defining a WHERE clause.

	CREATE TABLE BOOKS_PART(ISBN STRING,BOOKTITLE STRING,BOOKAUTHOR STRING,YEAR INT,PUBLISHER STRING)
	PARTITIONED BY (YR INT) 
	ROW FORMAT DELIMITED
	FIELDS TERMINATED BY '~'
	STORED AS TEXTFILE;
	
	INSERT INTO TABLE BOOKS_PART PARTITION (YR=1997) SELECT * FROM BOOKS WHERE YEAR=1997;
	
	SHOW PARTITIONS BOOKS_PART; <-- this will show the no of partitions available on the table

	'Export hive data into file' -> Manually create a part of data from hive to hdfs as below.
		INSERT OVERWRITE DIRECTORY '/user/hduser/external/books_part/yr=2004' 
		ROW FORMAT DELIMITED 
		FIELDS TERMINATED BY '~' 
		SELECT * FROM BOOKS WHERE YEAR=2004;

	move the o/p of above path to the location of the partitioned table & do an MSCK to automatically invoke the new partitions.

	'MSCK REPAIR' doesnt remove partitions if the corresponding folder on HDFS was manually deleted,  it will add any partitions that exist on HDFS but not in metastore to the metastore.

		MSCK REPAIR TABLE BOOKS_PART;
		OR 
		ALTER TABLE BOOKS_PART ADD PARTITION (YR=2004) LOCATION '/user/hduser/external/books_part/yr=2004';

	'Error statement' in MSCK execution:

	    0: jdbc:hive2:> msck repair table NCVDCH4_CVDP_INVALID_DATA_SEC_HTE;
		Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask (state=08S01,code=1)

		0: jdbc:hive2:> set hive.msck.path.validation;
		+----------------------------------+--+
		|               set                |
		+----------------------------------+--+
		| hive.msck.path.validation=throw  |
		+----------------------------------+--+
		
		To resolve the above issue follow the below:
		0: jdbc:hive2> set hive.msck.path.validation=ignore;
		
		0: jdbc:hive2> msck repair table cvdp_dev.NCVDCH4_CVDP_INVALID_DATA_SEC_HTE;
		No rows affected (0.609 seconds)

	'Adding multiple partitions in single query'
		ALTER TABLE EMPLOYEE ADD PARTITION (year=2005) PARTITION (year=2006) PARTITION (year=2008) PARTITION (year=2009) PARTITION (year=2010);
		ALTER TABLE db_bdpbase.Employee DROP IF EXISTS PARTITION (year=2008), PARTITION(year=2009), PARTITION(year=2010);
		ALTER TABLE db_bdpbase.Employee DROP IF EXISTS PARTITION(year>2010);

/*
if the underlying hdfs f/s was deleted without droping the table partition, we can refresh the metadata to reflect the current state by doing -- Drop the table (only in case of external table) - Recreate the table - Repair it (MSCK REPAIR TABLE table_name)
We can “statically” add a partition in the table and move the file into the partition of the table.
We can alter the partition in the static partition
*/

'Dynamic Partitioning'
	set hive.exec.dynamic.partition=true;
	set hive.exec.dynamic.partition.mode=nonstrict;
	set hive.exec.max.dynamic.partitions=1000
	set hive.exec.max.dynamic.partitions.pernode=100

 We can’t perform alter on the Dynamic partition.

	CREATE TABLE BOOK_EX(ISBN STRING, BOOKTITLE STRING, BOOKAUTHOR STRING, PUBLISHER STRING) --year column is not mentioned as a part of table column here
	PARTITIONED BY (YEAR INT)
	ROW FORMAT DELIMITED
	FIELDS TERMINATED BY '~'
	STORED AS TEXTFILE
	LOCATION '/user/hduser/external/book_xx'; 

	INSERT OVERWRITE TABLE BOOK_EX PARTITION(YEAR) SELECT * FROM BOOKS;
	/*
	To insert the data, the dynamic partition columns must be specified in last among the columns in the SELECT statement and in the same order in which they appear in the PARTITION() clause.
	*/
	'__hive_default_partition__' - This gets create when we load data into partitioned table dynamically and partition column has a NULL value. To check we can issue the query like below in where clause
		
		SELECT * FROM BOOK_EX WHERE YEAR="__HIVE_DEFAULT_PARTITION__";

====================
`Bucketing`
	set hive.enforce.bucketing = true ;

	We cannot load the data into a bucketed table using LOAD DATA INPATH command as Hive does not support it. 
	The no of reducers created at compile time and that will be equal to no.of buckets to gain equal distribution qualify the integer column which has high cardinal values example:empid

	--Create table movies
	CREATE TABLE MOVIES(MID INT,  MNAME STRING, GENRE array<STRING>)
	ROW FORMAT DELIMITED
	FIELDS TERMINATED BY ',' 
	COLLECTION ITEMS TERMINATED BY '|' 
	STORED AS TEXTFILE;

	--load from file /user/hduser/movies/movies.csv
	LOAD DATA LOCAL INPATH '/home/hduser/MV/movies.csv' INTO TABLE MOVIES;
	#above csv file has the header in the file -  alter the table to set new table property
	ALTER TABLE MOVIES SET TBLPROPERTIES ("skip.header.line.count"="1");
	-- to list the properties set fot the table.
	SHOW TBLPROPERTIES MOVIES;
	#to unset the tblproperties 
	ALTER TABLE MOVIES UNSET TBLPROPERTIES ('c', 'x', 'y', 'z'); --> c,x,y,z are all property names

	--after the alters performed again load the input .csv data 
	LOAD DATA INPATH '/user/hduser/movies/movies.csv' OVERWRITE INTO TABLE MOVIES;
	--create another table which is clustered and store in ORC format
	CREATE TABLE movies_orc_buc (MID INT,  MNAME STRING, GENRE ARRAY<STRING>)
	CLUSTERED BY (MID) 
	SORTED BY (MID) INTO 5 BUCKETS
	STORED AS ORC;
	--insert data to new table from the old table moveis
	INSERT INTO TABLE MOVIES_ORC SELECT * FROM MOVIES;
	--alter the buckets to 10 from 5
	ALTER TABLE MOVIE_ORC CLUSTERED BY (MID) INTO 5 BUCKETS;
	--again overwrite the insert from old table movies, Now we can see 10 buckets created in hdfs f/s

	'We can not set the table property 'transactional'='true' in an external table': --> below error will come
	/*
	Error: Error while processing statement: FAILED: Execution Error, return code 1 
	from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:cvdp.NCVDCDR_FORDPASS_APPR_LOC_SEC_HTE cannot be declared transactional because
	 its an external table) (state=08S01,code=1)
	*/


====================
`JOINS in hive`

'NORMAL JOIN':
	SELECT c.ID, c.NAME, o.AMOUNT, o.DATE FROM CUSTOMERS c LEFT|RIGHT|FULL OUTER JOIN ORDERS o ON (c.ID = o.CUSTOMER_ID);

	Normal join works - each table goes into separate mappers and then to shuffle phase - then to reducer where as one mapper output is copied 
	in to memory and another mapper out is streamed to the reducer to perform join. So it is better to load the bigger table to stream and small 
	table to copied to memory. 

'STREAMTABLE HINT':
	Select /*+ STREAMTABLE(a) */ a.key, a.value from a join b on a.key = b.key;
	
	if table a is bigger we are telling hive to stram the table a whereas b copied to memory. Thus enabling optimization. Still stramtable hint uses reducer to perform joins.
	By default the tables in the left is copied to memory and right side table goes to stream.

'MAP JOIN':

	Map Side Join since one of the tables in the join is a small table and can be loaded into memory, so that a join could be performed within A MAPPER without using a Map/Reduce step.
	Rightouter join and full outer join are not possible in MAPSIDE join.

'MAPJOIN HINT':
	Select /*+ MAPJOIN(b) */ a.key, a.value from a join b on a.key = b.key <-- MAPJOIN(b) tells the hive to choose b to load in to memory.
	make sure the parameter set hive.ignore.mapjoin.hint=true is set to true. 

	MAPJOIN creates a MAPRED LOCAL TASK is launched before the original join task and serializes the table in hint into an hashtable, 
	the hash table is then uploaded to HADOOPs DISTRIBUTED CACHE - hadoop will copy the hashtable to the local f/s to all the nodes executing the actual task before anytask 
	for the job are executed on the node. HASHTABLE will be available locally to each mappers. Then the actual join performed on the mapper.

'AUTO JOIN:'
	Auto Join creates a conditional task to check which table in the join has largedataset based on it, Hive chooses table to placed in DC.

	Parameters: 
	hive.auto.convert.join=true <-- def is true, if set automatically process all joins as mapside join if a table size is less than 25 MB.
	hive.mapjoin.smalltable.filesize=25000000 <-- def is 25MB this size limits the auto join, if we want to enable mapjoin automatically for tables are of size more than 25MB we can tune this paramater.
	hive.auto.convert.join.noconditionaltask=true <-- we can combine three or more map-side joins into a single map-side join if the size of the n-1 table is less than 10 MB.
	hive.auto.convert.join.noconditionaltask.size=10000000; <-- by def 10MBthis size limits the noconditionaltask parameter to work

'SMB (Sort-Merge-Bucketed) JOIN':
   To perform SMB join all the following criteria must be true
	1. All join tables must be bucketized.
	2. Number of buckets of the big table must be divisible by the number of buckets of each of the small table in your query.
	3. Bucket columns from the table and the join columns in your query must be the one and the same.
	4. Following properties must be setup 

	set hive.auto.convert.sortmerge.join=true;
	set hive.optimize.bucketmapjoin = true;
	set hive.optimize.bucketmapjoin.sortedmerge = true;
	set hive.auto.convert.sortmerge.join.noconditionaltask=true;

	Big table will result in 10 mappers if the no of bucket is 10. 
	only matching bucket of all small tables are replicated on to each mapper and then the join will takes place on each mapper. 
	Since the records are already sorted, only matching buckets are joined - this task requires no reducers and completes faster.

'For Historical LOAD': If any historical load occurs (Join single or multiple tables or one hive table to another table load) in your feature through one time Hive script, 
	please include the below hive settings to avoid the memory issue/script interruption in middle.

	set tez.am.resource.memory.mb=8192;
	set hive.exec.orc.split.strategy=BI;
	set hive.orc.cache.use.soft.references=true;

	set hive.exec.orc.split.strategy=BI;

	if your source table is orc and splits calculation takes too long time need to add above settings

	set hive.orc.cache.use.soft.references=true;

	Setting this to true can help avoid out-of-memory issues under memory pressure (in some cases) at the cost of slight unpredictability in overall query performance.

====================
`Built-in Table-Generating Functions (UDTF)`
	Normal user-defined functions, such as concat(), len() take in a single input row and output a single output row. In contrast, table-generating functions transform a single input row to multiple output rows.

'EXPLODE' -> (array and map)
	Lateral view explode, explodes the array data into multiple rows. Like a cross join in the example below:

	CREATE TABLE test_l7 (person string, phones array<int> ) row format delimited fields terminated by ',' STORED AS TEXTfile LOCATION '/hdfs/path/testing_l7';
	 
	insert into test_l7 select  'lingesh' , ARRAY( 96771 , 96771090 , 3 );
	insert into test_l7 select  'prashanth' , ARRAY( 86771 , 971090 , 2 );
	insert into test_l7 select  'sudharsan' , ARRAY( 76771 , 96770 , 1 );
	insert into test_l7 select  'ramesh' , ARRAY( 66771 , 960 , 8783 );
	insert into test_l7 select  'subash' , ARRAY( 56771 , 96 , 83783 );
	insert into test_l7 select  'mani' , ARRAY( 46771 , 90 , 783 );

	select * from test_l7;
	+------------+---------------------------+
	|   person   |          phones           |
	+------------+---------------------------+
	| prashanth  | [86771,971090,989838783]  |
	| sudharsan  | [76771,96770,989837883]   |
	| ramesh     | [66771,960,8783]          |
	| subash     | [56771,96,83783]          |
	| mani       | [46771,90,783]            |
	| lingesh    | [96771,96771090,3]        |
	| prashanth  | [86771,971090,2]          |
	| sudharsan  | [76771,96770,1]           |
	+------------+---------------------------+

Applying a lateral view explode on the above table will expand the both person and phones & do a cross join, your final table will look like this:
	
	select person, Phone 
	from test_l7 
	lateral view explode(phones) pn as Phone;

	+------------+------------+
	|   person   |   phone    |
	+------------+------------+
	| prashanth  | 86771      |
	| prashanth  | 971090     |
	| prashanth  | 989838783  |
	| sudharsan  | 76771      |
	| sudharsan  | 96770      |
	.
	.
	| sudharsan  | 76771      |
	| sudharsan  | 96770      |
	| sudharsan  | 1          |
	+------------+------------+
	24 rows selected (0.365 seconds)

--------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------
'POSEXPLODE' -> (array only)

	CREATE TABLE testpos( person string, phones array<int>, country array<string> ) row format delimited fields terminated by ',' STORED AS TEXTfile  LOCATION 'path/hdfs/testpos';
	 
	insert into testpos select  'lingesh' , ARRAY(96771, 96771090, 3), ARRAY('IND','USA','CHN');
	insert into testpos select  'prashanth' , ARRAY(86771, 971090, 2), ARRAY('NRI','AED','RSA');
	insert into testpos select  'sudharsan' , ARRAY(76771, 96770, 1), ARRAY('DOG','CAT','PLI');
	insert into testpos select  'ramesh' , ARRAY(66771, 960, 8783), ARRAY('BAR','RAP','JOK');
	insert into testpos select  'subash' , ARRAY(56771, 96, 83783), ARRAY('DHL','CHN','BOM');
	insert into testpos select  'mani' , ARRAY(46771, 90, 783), ARRAY('ISU','BOB');

	select * from testpos;
	+------------+---------------------+----------------------+
	|   person   |       phones        |       country        |
	+------------+---------------------+----------------------+
	| lingesh    | [96771,96771090,3]  | ["IND","USA","CHN"]  |
	| prashanth  | [86771,971090,2]    | ["NRI","AED","RSA"]  |
	| sudharsan  | [76771,96770,1]     | ["DOG","CAT","PLI"]  |
	| ramesh     | [66771,960,8783]    | ["BAR","RAP","JOK"]  |
	| subash     | [56771,96,83783]    | ["DHL","CHN","BOM"]  |
	| mani       | [46771,90,783]      | ["ISU","BOB"]        |
	+------------+---------------------+----------------------+

"POSEXPLODE gives you an index along with value when you expand any column, and then you can use this indexes to map values with each other as mentioned below."

	SELECT person, pos_phone, phones_, pos_country, country_ <-- selecting the positional fields for reference
	FROM testpos 
	lateral view posexplode(phones) pn AS pos_phone , phones_ <-- produces (0, 96771)
	lateral view posexplode(country) cn AS pos_country , country_ <-- produces (0, IND)
	WHERE pos_phone == pos_country;  <-- join condition on the positional index values

	+------------+------------+-----------+--------------+-----------+
	|   person   | pos_phone  |  phones_  | pos_country  | country_  |
	+------------+------------+-----------+--------------+-----------+
	| lingesh    | 0          | 96771     | 0            | IND       |
	| lingesh    | 1          | 96771090  | 1            | USA       |
	| lingesh    | 2          | 3         | 2            | CHN       |
	| prashanth  | 0          | 86771     | 0            | NRI       |
	| prashanth  | 1          | 971090    | 1            | AED       |
	| prashanth  | 2          | 2         | 2            | RSA       |
	| sudharsan  | 0          | 76771     | 0            | DOG       |
	| sudharsan  | 1          | 96770     | 1            | CAT       |
	| sudharsan  | 2          | 1         | 2            | PLI       |
	| ramesh     | 0          | 66771     | 0            | BAR       |
	| ramesh     | 1          | 960       | 1            | RAP       |
	| ramesh     | 2          | 8783      | 2            | JOK       |
	| subash     | 0          | 56771     | 0            | DHL       |
	| subash     | 1          | 96        | 1            | CHN       |
	| subash     | 2          | 83783     | 2            | BOM       |
	| mani       | 0          | 46771     | 0            | ISU       |
	| mani       | 1          | 90        | 1            | BOB       |
	+------------+------------+-----------+--------------+-----------+

--------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------
'INLINE (array of structs)'
	Explodes an array of structs to multiple rows. Returns a row-set with N columns (N = number of top level elements in the struct).

Create a table with struct type columns in it and insert corresponding values:

	CREATE TABLE cvdp.test_st ( person struct<id : int, name: string, organization : string>, activity struct<id : int, name: string>, cas struct<caseid : int, casename: string, caseorganization : string> )
	ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'  OUTPUTFORMAT  'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat' 
	LOCATION '/hdfs/path/testing_st' ;

"NAMED_STRUCT is another function used to insert a value maually on array(struct<>) type columns"
	INSERT INTO test_st SELECT NAMED_STRUCT('id',1,'name', 'pras', 'organization', 'it'),  NAMED_STRUCT('id',2,'name', 'pras2'), NAMED_STRUCT('caseid',3,'casename', 'pras3', 'caseorganization', 'it3') ;

Actual data in the table is as below:
	SELECT * FROM test_st;
	+---------------------------------------------+--------------------------+----------------------------------------------------+
	|                   person                    |         activity         |                        cas                         |
	+---------------------------------------------+--------------------------+----------------------------------------------------+
	| {"id":1,"name":"pras","organization":"it"}  | {"id":2,"name":"pras2"}  | {"caseid":3,"casename":"pras3","caseorganization":"it3"} |
	+---------------------------------------------+--------------------------+----------------------------------------------------+
	1 row selected (1.115 seconds)

Use Inline function to flatten the structure:
 
	SELECT pn.* , an.* , cn.* from test_st
	LATERAL VIEW INLINE(array(person)) pn
	LATERAL VIEW INLINE(array(activity)) an
	LATERAL VIEW INLINE(array(cas)) cn;
	+-----+-------+---------------+-----+--------+---------+-----------+-------------------+
	| id  | name  | organization  | id  |  name  | caseid  | casename  | caseorganization  |
	+-----+-------+---------------+-----+--------+---------+-----------+-------------------+
	| 1   | pras  | it            | 2   | pras2  | 3       | pras3     | it3               |
	+-----+-------+---------------+-----+--------+---------+-----------+-------------------+

--------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------
'JSON_TUPLE'
	It takes a set of names (keys) and a JSON string, and returns a tuple of values using one function. This is much more efficient than calling GET_JSON_OBJECT to retrieve more than one key from a single JSON string.

Example table has a json type in a column which is stored as string datatype:

select cvdpx3_raw_x from NCVDPX3_ETISIDS_PART_NUM_PII_HTE limit 2;
	+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--+
	|                                                                                                                           cvdpx3_raw_x                                                                                                                            |
	+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--+
	| {"isConfig":"false","identifier":"\"F163\"","session_Open_Local":"\"2018-04-03T13:58:27+02:00\"","session_Open_UTC":"\"2018-04-03T13:58:27+02:00\"","ecu_name":"\"BCMB\"","VIN":"\"1FA6P8CF0F5360021\"","partNumber":"3","didType":"\"ECU_DIAGNOSTIC_SPEC\""}     |
	| {"isConfig":"false","identifier":"\"F110\"","session_Open_Local":"\"2018-04-03T13:58:27+02:00\"","session_Open_UTC":"\"2018-04-03T13:58:27+02:00\"","ecu_name":"\"ABS\"","VIN":"\"1FA6P8CF0F5360021\"","partNumber":"\"DSGR3C-2C219-BA\"","didType":"\"OTHER\""}  |
	+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--+

select a.cvdpx3_sha_k, b.* from NCVDPX3_ETISIDS_PART_NUM_PII_HTE a lateral view json_tuple(a.cvdpx3_raw_x, 'identifier', 'ecu_name') b as f1, f2 limit 2;
	+-------------------------------------------------------------------+---------+---------+--+
	|                           cvdpx3_sha_k                            |   f1    |   f2    |
	+-------------------------------------------------------------------+---------+---------+--+
	| 0116c2761be5c61020d5205e737eec4385581bb260f7f0ee3a50e98298a2249b  | "F163"  | "BCMB"  |
	| 014918d61cd288be0d61ff9bbb4feb6c3746129fb5af91c53b98a09e3164f981  | "F110"  | "ABS"   |
	+-------------------------------------------------------------------+---------+---------+--+

'get_json_object' <-- general string function
	
The above desired output can be formed using this function which has more lines and needs to be called everytime to get every sinlge value from JSON.

select a.cvdpx3_sha_k, get_json_object(a.cvdpx3_raw_x, '$.identifier') as f1 , get_json_object(a.cvdpx3_raw_x, '$.ecu_name') as f2 from NCVDPX3_ETISIDS_PART_NUM_PII_HTE a limit 2;
	+-------------------------------------------------------------------+---------+---------+--+
	|                           cvdpx3_sha_k                            |   f1    |   f2    |
	+-------------------------------------------------------------------+---------+---------+--+
	| 0116c2761be5c61020d5205e737eec4385581bb260f7f0ee3a50e98298a2249b  | "F163"  | "BCMB"  |
	| 014918d61cd288be0d61ff9bbb4feb6c3746129fb5af91c53b98a09e3164f981  | "F110"  | "ABS"   |
	+-------------------------------------------------------------------+---------+---------+--+
	2 rows selected (0.135 seconds)

====================
`SET PARAMETERS - HIVE`

"Eliminates map and Reduce jobs" - Hive queries can utilize fetch task (directly read from disk), which can avoid the overhead of starting MapReduce job.
	
	'hive.fetch.task.conversion': This parameter controls which kind of simple query can be converted to a single fetch task, thus eliminating Map jobs.
	
		It was added in Hive 0.10 per HIVE-2925. 
	    Value "none" is added in Hive 0.14 to disable this feature, per HIVE-8389.
	    Value "minimal" means SELECT *, FILTER on partition columns (WHERE and HAVING clauses), LIMIT only.
	    Value "more" means SELECT, FILTER, LIMIT only (including TABLESAMPLE, virtual columns)."more" can take any kind of expressions in the SELECT clause, including UDFs.

	    We can see "Fetch Operator" being used in explain plans under "STAGE PLANS" section if this parameter is enabled. 
	    "NOTE: fetch can not utilize the parallelism of MapReduce framework."

	'hive.fetch.task.conversion.threshold' : this parameter controls input threshold (in bytes) for applying hive.fetch.task.conversion
											 The default value was changed in Hive 0.14 to 1GB(1073741824). To disable this feature set the values as -1.
		
		If hive.fetch.task.conversion.threshold is less than the table size, it will use MapReduce Job (in otherwords, if table size is greater than 1G (defaultvalue) use Mapreduce instead of Fetch task.)


    'hive.fetch.task.aggr': Eleminates the reduce job when using "aggregation queries with no group-by clause" (for example, select count(*) from src) execute final aggregations in a single reduce task. 
    						If this parameter is set to true, Hive delegates the final aggregation stage to a fetch task, possibly decreasing the query time.

	    > set hive.fetch.task.aggr; 
		+-----------------------------+--+
		|             set             |
		+-----------------------------+--+
		| hive.fetch.task.aggr=false  |		<-- initial value is false
		+-----------------------------+--+
		
	    > select count(*) from NCVDPX3_ETISIDS_PART_NUM_PII_HTE;
		--------------------------------------------------------------------------------
		        VERTICES      STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED
		--------------------------------------------------------------------------------
		Map 1 ..........   SUCCEEDED      5          5        0        0       0       0
		Reducer 2 ......   SUCCEEDED      1          1        0        0       0       0  <-- Usgae of reducers 
		--------------------------------------------------------------------------------
		VERTICES: 02/02  [==========================>>] 100%  ELAPSED TIME: 7.43 s
		--------------------------------------------------------------------------------
		+------+--+
		| _c0  |
		+------+--+
		| 134  |
		+------+--+
		1 row selected (8.156 seconds)
		
		> set hive.fetch.task.aggr=true; <-- enabling by setting the value to true
		No rows affected (0.004 seconds)
		
		> select count(*) from NCVDPX3_ETISIDS_PART_NUM_PII_HTE;
		--------------------------------------------------------------------------------
		        VERTICES      STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED
		--------------------------------------------------------------------------------
		Map 1 ..........   SUCCEEDED      5          5        0        0       0       0
		--------------------------------------------------------------------------------	<-- No reducers used 
		VERTICES: 01/01  [==========================>>] 100%  ELAPSED TIME: 11.44 s
		--------------------------------------------------------------------------------
		+------+--+
		| _c0  |
		+------+--+
		| 134  |
		+------+--+
		1 row selected (12.189 seconds)

	"Note, if the query has "group-by", it can not use this feature"

	



=============================================================================
`SCALA`

scala -version --> to verify 

val -- declare a immutable variable
var -- declare a mutable variable. the variable can be reassigned
lazy val -- the value to the variable will not assigned until it is being called. same as lazy evaluation 
		 |--use to save memory to declared prior 


object MyFuncs {
	def sum(func: Int => Int, lb: Int, ub: Int)  = {
  	var t= 0
  	for (i <- lb to ub) {
  		t += func(i)
  		}
	t
	}

	def sqr (i: Int) = i * i
	def cube (i: Int) = i * i * i
}

scala> case class Odc(oid: Int, odt: String,cid: Int,ost: String) {
     | println("hello")
     | }
defined class Odc

scala> val newOdc = new Odc(1,"2018-10-10",100,"COMPLETE")
hello
newOdc: Odc = Odc(1,2018-10-10,100,COMPLETE)

scala> newOdc.oid
res16: Int = 1

---
Create a list of values - square them and add them
l: List[Int] = List(1, 2, 3, 4, 5)

scala> val f = l.map(xx => xx * xx)
f: List[Int] = List(1, 4, 9, 16, 25)

scala> val m = f.reduce((x,y) => x+y)
m: Int = 55

functions:

'MONAD'  -->https://medium.com/@sinisalouc/demystifying-the-monad-in-scala-cc716bb6f534
'option'
'trait'
:+

===============================================================
`SPARK`:
========
'Installation':
	Download tar file http://mirrors.wuchna.com/apachemirror/spark/spark-2.3.2/spark-2.3.2-bin-hadoop2.7.tgz
		tar -xf spark-2.3.2-bin-hadoop2.7.tgz
		sudo mv  spark-2.3.2-bin-hadoop2.7 /usr/local/spark
		ls -ld /usr/local/spark
		vi ~/.bashrc --> add following lines
	Scala Path
		export SPARK_HOME=/usr/local/scala
		export PATH=$PATH:$SPARK_HOME/bin
		
	Spark Master Configuration:
		cd /usr/local/spark/conf
		cp spark-env.sh.template spark-env.sh
		
	vi /usr/local/spark/conf/spark-env.sh --> add following lines
		export SPARK_MASTER_HOST=master
		export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_181
	
	vi /usr/local/hadoop/etc/hadoop/yarn-site.xml
		<property>
		 <name>yarn.nodemanager.vmem-check-enabled</name>
		 <value>false</value>
		 <description>Whether virtual memory limits will be enforced for containers</description>
		</property>
		<property>
		 <name>yarn.nodemanager.vmem-pmem-ratio</name>
		 <value>4</value>
		 <description>Ratio between virtual memory to physical memory for containers</description>
		</property> 


	hadoop fs -put /usr/local/spark/jars/* /user/spark/share/lib/     -- */

	spark.yarn.memory=256m
	spark.driver.memory=25m
	spark.driver.cores=1
	spark.executor.cores=1
	spark.executor.memory=256m
	spark.yarn.jars=hdfs://master:54310/user/spark/share/lib/*.jar      ---- */

spark-shell --verbose --master yarn --driver-memory 512m --num-executors 1 --executor-cores 2 --executor-memory 512m

'Error while starting spark-shell':
	ERROR TransportClient:233 - Failed to send RPC
	refer --> https://issues.apache.org/jira/browse/YARN-4714 
	'Solution':
		spark-shell --master yarn --conf "spark.executor.extraJavaOptions = -XX:ReservedCodeCacheSize=100M -XX:MaxMetaspaceSize=256m -XX:CompressedClassSpaceSize=256m" driver-memory 512m --num-executors 1 --executor-cores 2 --executor-memory 512m --conf "spark.yarn.jars=hdfs://master:54310/user/spark/share/lib/*.jar" --conf "spark.yarn.maxAppAttempts=1"

spark -version -> to find the version of the spark

//Initialize programatically - v1.6
	import org.apache.spark.{SparkConf, SparkContext}
	val conf = new SparkConf().setAppName("anyName").setMaster("yarn")
	val sc = new SparkContext(conf)

//To create SqlContext 
	scala> import org.apache.spark.sql
	scala> val sqlc = new org.apache.spark.sql.SQLContext(sc)
	sqlc: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@614c7175

//To list the environment details of spark which is already running 
	sc.getConf.getAll
	sc.getConf.getAll.foreach(println)
=====
`RDD (resilient distributed datasets)`
===== RDD is an immutable collection of distributed objects/elements partitioned across the nodes of clusters and operated on in parallel. Features are immutable, inmemory computation, lazy evaluations, Fault tolerance, Partitioning. 

	RDD operation: Transformations - Creating a new RDD from an existing RDD is known as transformation. Ex: map, flatMap
		Actions - all the transformations are triggered only when an action is called. Ex: sum, count.
		
`RDD Creation` 
	'Using parallized collection' - it can be created by applying parallelize method on an existing collection.
		val data = Array(1,2,3,4,5)
		val rddData = sc.parallelize(data)
		rddData.collect() 

	'Using External Sources' - Use textFile() of SparkContext.
		val rdd = sc.textFile("hdfs://localhost:9000/data/keywords.txt")

	'Read data from HDFS' - it creates a RDD
		scala> val orders = sc.textFile("/user/hduser/retail_db/orders")
			orders: org.apache.spark.rdd.RDD[String] = /user/hduser/retail_db/orders MapPartitionsRDD[27] at textFile at <console>:25

	'Read data from Linux FS' 
		scala> val ordersRaw = scala.io.Source.fromFile("/home/hduser/retail_db/orders") <-- Itenory type - to change this to RDD use parallelize method.
		scala> val ordersRDD = sc.parallelize(ordersRaw)
				
	'Creating an RDD with file Partitioning':  By default, it divides data into two partitions also the num of partitions is equal to the num of CPU cores available in the cluster, but the number of partitions can be specified while creating an RDD.
		val rdd = sc.textFile("home/data/keywords.txt", 3)

`Transformations`
	'map'- acts on one element and produces one element. Returns a new data set by operating on each element of the source RDD.
		MAP o/p is of type - org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[8]

		val orderDate = orders.map(mm => {
     	| (mm.split(",")(1).substring(0, 10).replace("-", "").toInt, mm.split(",")(0).toInt)
     	| })

   	'flatmap' - acts on one element and produces 0 or 1 or more elements and it flattens list of lists into a single list
     	 	flatmap o/p is of type -   org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[9] at flatMap
		val l = List("Hello", "How are you doing", "Let us perform copy","dont copy", "as part of count program","how is everything")
		val lst = List("Hello,Hello", "Hello,Hello")
		val lstRdd = sc.parallelize(lst)
		val lstFm = lstRdd.flatMap(mm => mm.split(","))
		val wc = lstFm.map(mm => (mm, 1)).countByKey

	'filter' -  Returns a new RDD that contains only elements that satisfy the condition.
		val rdd = sc.textFile("/home/data/keywords.txt")
		val filterRdd = rdd.filter(line => line.contains("Kafka"))
		filterRdd.collect()
		
	'joinfunc' - Join needs a paired RDD to join two dataset. key value (primarykey) should be similar

		val joi = sc.textFile("/user/hduser/retail_db/order_items")
		val ordersMap = jo.map(mm => (mm.split(",")(0).toInt, mm.split(",")(1).substring(0,10)))
		val orderItemMap = joi.map(mm => (mm.split(",")(1).toInt,mm.split(",")(4).toFloat))
		val join = ordersMap.join(orderItemMap)
		join.take(10).foreach(println)

	'leftouterjoin'
		val leftOuterJoin = ordersMap.leftOuterJoin(orderItemMap)
		val leftouterjoinfilter = leftOuterJoin.filter( mm => mm._2._2 == None)
		val final = leftouterjoinfilter.map(mm => mm._2._1)

	
`Actions`: Action returns values to the driver program.

	'reduce(func)' - returns a data set by aggregating the elementsof the data set using a function func. 
			The function takes two arguments and returns a single argument.
		val rdd = sc.parallelize(1 to 5) 
		val sumRdd = rdd.reduce((t1,t2) => t1 + t2)
		
	'collect()' - All the elements of the data set are returned as an array to the driver program. Collect will convert the distributed RDD in to a single threaded collection. Collect will attempt to copy every single element in the RDD onto the single driver program, and then run out of memory and crash. use take() or takeSample() api as collect will cause memory realated issue as collect api uses heavy memory.
	
	'count()' - This returns the number of elements in the data set.
	
	'first()' -  This returns the first element in the data set.
	
	'take(n)' - This returns the first n elements in the data set as an array.
	
	'saveAsTextFile(path)' - Write the elements of the RDD as a text file in the local file system, HDFS, or another storage system.
	
	'foreach' - If we apply foreach directly on RDD - it will not return anything. Because spark apis will be executed in worker nodes where the apis we are performing on the arrays will be excuted in current session/machine where we running it.

`Paired RDDs`:
	 Each element in the pair RDDs is represented as a key/value pair. Pair RDDs are useful for sorting, grouping.

	'groupByKey' - doesnt take any arguments to be passed, doesnot use combiner 
			In groupByKey(), all the key-value pairs are shuffled around. This is a lot of unnecessary data to being transferred over the network.
		val words = Array("one", "two", "two", "three", "three", "three")
		val wordPairsRDD = sc.parallelize(words).map(word => (word, 1))
		val wordCountsWithGroup = wordPairsRDD
		.groupByKey()
		.map(t => (t._1, t._2.sum))
		.collect()
		
	'reduceByKey' - works much better on a large dataset. Thats because Spark knows it can combine output with a common key on each partition before shuffling the data. pairs on the same machine with the same key are combined (by using the lamdba function passed into reduceByKey) before the data is shuffled. Then the lamdba function is called again to reduce all the values from each partition to produce one final result
		val words = Array("one", "two", "two", "three", "three", "three")
		val wordPairsRDD = sc.parallelize(words).map(word => (word, 1))
		val wordCountsWithReduce = wordPairsRDD
		.reduceByKey(_ + _)
		.collect()
	
	'combineByKey' - It can be used when you are combining elements but your return type differs from your input value type.
	
	'sortByKey()' - applied on a data set of (K, V) pairs, it returns a data set of (K, V) pairs where keys are sorted in ascending or descending order as specified in the boolean ascending argument. 
		val rdd = sc.textFile("/home/data/people.csv")
		val splitRdd = rdd.filter(line => !line.contains("year")).map(line => line.split(","))
		val fieldRdd = splitRdd.map(f => (f(1),f(3).toInt)).sortByKey(false) <- false represents that the data to be sorted in desc order
		fieldRdd.foreach(println)
	#
	
`Lineage` -  RDD lineage is nothing but the graph of all the parent RDDs of an RDD. it creates a logical execution plan.
			To view the lineage, use toDebugString
		>> rdd.toDebugString
		
`DAG - 'Direct Acylic Graph'`:
	 Phy exec plan.  When an action is called on the RDD, Spark creates the DAG and submits the DAG to the DAG scheduler.
		1. The DAG scheduler divides operators such as map, flatMap, and so on, into stages of tasks.
	 	2. The result of a DAG scheduler is a set of stages.
		3. The stages are passed on to the Task Scheduler.
		4. The Task Scheduler launches tasks via Cluster Manager.
		5. The worker executes the tasks.
	At a high level, Spark applies two transformations to create a DAG. The two transformations are as follows:
		• 'Narrow transformation:' The operators that don’t require the data to be shuffled across the partitions are grouped together as a stage.
			Examples are map, filter, and so on.
		• 'Wide transformation:' The operators that require the data to be shuffled are grouped together as a stage. An example is reduceByKey

`Persisting RDD`:  Persisting an RDD stores the computation result in memory and reuses it in other actions on that data set. 
	'cache()' - Uses the default storage level is StorageLevel.MEMORY_ONLY -> RDD is stored as deserialized Java object in the JVM. If the size of RDD is 			greater than memory, It will not cache some partition and recompute them next time whenever needed. 
			
	'persist()' - we can use various storage levels in this method as follows 
		1. MEMORY_AND_DISK -> 'When the size of RDD is greater than the size of memory, it stores the excess partition on the disk, and retrieve from disk whenever required',
		2. MEMORY_ONLY_SER, 3. MEMORY_AND_DISK_SER, 4. DISK_ONLY
		
		>> val units = words.map ( word => (word, 1) ).persist(StorageLevel.MEMORY_ONLY) or
		>> val units = words.map ( word => (word, 1) ).cache
	
	'unpersist()' - Spark monitor the cache of each node automatically and drop out the old data partition in the LRU (least recently used) fashion.
					We can also remove the cache manually using RDD.unpersist() method.

	#Refer ( https://data-flair.training/blogs/apache-spark-rdd-persistence-caching/ ) 

==========
`SparkSQL` - The Spark SQL module consists of DataFrames and Datasets and Catalyst Optimizer.
==========	
`DataFrame` - A DataFrame is an immutable, distributed collection of data that is organized into rows, where each one consists set of columns which has name and type. In other words, this distributed collection of data has a structure defined by a schema. 

`Creating DataFrames`
	'Creating DataFrames from RDDs': 
		"To use toDF function we need to import implicit modules" . 
		IMPLICITS - Used for converting Scala objects (incl. RDDs) into a Dataset, DataFrame, Columns or supporting such conversions. implicits object is defined inside SparkSession and hence requires that you build a SparkSession instance first before importing implicits conversions.
		
		>> import org.apache.spark.sql.SparkSession
		>> val spark: SparkSession = new SparkSession
									.builder
									.master("local[*]")
									.enableHiveSupport()
									.config("spark.sql.warehouse.dir", "target/spark-warehouse")
									.setAppName("someName")
									.getOrCreate()
		>> import spark.implicits._
		>> val df = Seq("SOME DATA!").toDF("columnName")
		>> val df2 = sc.parallelize(Seq("hello, I'm a very low-level RDD")).toDF
		>> import scala.util.Random
		>> val rdd2 = spark.sparkContext.parallelize(1 to 10).map(x => (x,Random.nextInt(100)* x))
		>> val kvDF = rdd2.toDF("key","value")
	Printing the Schema and Showing the Data of a DataFrame
		>> kvDF.printSchema
			|-- key: integer (nullable = false)
			|-- value: integer (nullable = false)
		>> kvDF.show(2)
			+---+-----+
			|key|value|
			+---+-----+
			| 1| 58	  |
			| 2| 18   |
			+---+-----+
		>> spark.range(5,15,2).toDF("num").show  //spark.range itself results in RDD
		
	'Converting a Collection Tuple to a DataFrame Using Spark’s toDF Implicit':
		>> val movies = Seq(("Damon, Matt", "The Bourne Ultimatum", 2007L),
							("Damon, Matt", "Good Will Hunting", 1997L))
		>> val moviesDF = movies.toDF("actor", "title", "year")
		>> moviesDF.printSchema
			|-- actor: string (nullable = true)
			|-- title: string (nullable = true)
			|-- year: long (nullable = false)
		>> moviesDF.show
			+-----------+--------------------+----+
			| actor		| title				 |year|
			+-----------+--------------------+----+
			|Damon, Matt|The Bourne Ultimatum|2007|
			|Damon, Matt| Good Will Hunting  |1997|
			+-----------+--------------------+----+
			
	'Creating a DataFrame from an RDD with a Schema Created Programmatically'
		>> import org.apache.spark.sql.Row
		>> import org.apache.spark.sql.types._		<-- required to using types in schema creation
		>> val peopleRDD = spark.sparkContext.parallelize(Array(Row(1L, "John Doe", 30L), Row(2L, "Mary Jane", 25L)))
		>> val schema = StructType(Array(
					   StructField("id", LongType, true),
					   StructField("name", StringType, true),
					   StructField("age", LongType, true) ))
		>> val peopleDF = spark.createDataFrame(peopleRDD, schema)
	Displaying the Schema of peopleDF and Its Data
		>> peopleDF.printSchema
		|-- id: long (nullable = true)
		|-- name: string (nullable = true)
		|-- age: long (nullable = true)
		>> peopleDF.show
		+--+-----------+---+
		|id| name     |age |
		+--+-----------+---+
		| 1| John Doe | 30 |
		| 2| Mary Jane| 25 |
		+--+-----------+---+
		
	'Creating DataFrames from Data Sources':
		General format - >> spark.read.format(...).option("key", "value").schema(...).load()
	'txt file' 
		#reading data from text file separated by | symbol and creating DF on it. Considering the 3rd column is Integer (total 4 columns).
    	>> val rawInput = sc.textFile("text.txt")
	
    	>> val df1= rawInput.map(x => x.split("|")).map( x=> (x(0),x(1),x(2).toInt,x(3)) ).toDF("tbl_nm", "plnt_nm", "pk_nm", "log")
    	>> val df2 = rawInput.map(mm => mm.split("|")).map {case Array(a,b,c,d) => (a,b,c.toInt,d)}.toDF("tbl_nm","plnt_nm","pk_nm","log")
    	 
    	>> case class Schema(tbl_nm: String, plnt_nm: string, pk_nm: Int, log: string)
    	>> val df3= rawInput.map(x => x.split("|")).map( x=> Schema(x(0),x(1),x(2),x(3)) ).toDF()
    	 
    	>> import org.apache.spark.sql.types.{StructType,StructField,StringType}
    	>> val schema = StructType(Array(
    	        StructField("id", LongType, true),
    	        StructField("name", StringType, true),
    	        StructField("age", LongType, true) ))
    	>> val df4 = spark.createDataFrame(rawInput, schema)
    	
    	>> df4.printSchema
    	 
    We use 'registerTempTable' or CreateOrReplaceTempView/createGlobalTempView (Spark > = 2.0) on our spark Dataframe.
      'CreateorReplaceTempView' is used when you want to store the table for a particular spark session and 
      'createGlobalTempView' is used when you want to share the temp table across multiple spark sessions.
    
    	>> df1.registerTempTable("SampleTable")
       	>> sqlContext.sql("select count(1) from SampleTable").show
    
    Inspect RDD Partitions Programatically
    	In the Scala API, an RDD holds a reference to its Array of partitions, which you can use to find out how many partitions there are:
    
    	>> scala> val someRDD = sc.parallelize(1 to 100, 30)
    	   someRDD: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:12
    
    	>> scala> someRDD.partitions.size
    	   res0: Int = 30
    	In the python API, there is a method for explicitly listing the number of partitions:
    
    	>> scala> someRDD = sc.parallelize(range(101),30)
    
    	>> scala> someRDD.getNumPartitions()
    	>> res1: 30

	'CSV format'
		>> import org.apache.spark.sql.types._
		>> val movieSchema = new StructType()
						.add("actor",SringType,true)
						.add("title",StringType,true)
						.add("year",IntegerType,true)
		>> val movies4 = spark.read.option("header","true").option("sep", "\t").option("mode", "PERMISSIVE").schema(movieSchema).csv("<path>/movies.csv")
		
	"inferSchema" option -  specifies whether Spark should try to infer the column type based on column value. Or we can define user-specified-schema using StructType and StructField & specify in schema option as above line. If the inferSchema option is false and no schema is provided, Spark will assume the data type of all the columns to be the string type.
		
	"Verify correctness of the data":
		'PERMISSIVE (default)': nulls are inserted for fields that could not be parsed correctly.
			We can add a special column _corrupt_record, which does not exist in the data. This column captures rows that did not parse correctly. To Query which rows not parsed correctly we can use the below codes:
				>> spark.read.schema(schema).csv(file).filter($"_corrupt_record".isNotNull).count()
				>> spark.read.schema(schema).csv(file).select("_corrupt_record").show()
		'DROPMALFORMED': drops lines that contain fields that could not be parsed
		'FAILFAST': aborts the reading if any malformed data is found
			
	'JSON file'
		>> val orders = sqlContext.load("/user/hduser/retail_db_json/orders", "json")
			orders: org.apache.spark.sql.DataFrame = [order_customer_id: bigint, order_date: string ... 2 more fields]
		"JSON Common Options" -> allowComments, multiLine, samplingRatio  	
		>> val movies5 = spark.read.option("multiLine","true").option("inferSchema","true").schema(movieSchema2).json("<path>/movies.json")  //specify a schema to override the Sparks inferring schema.
	'When a column data type specified in the schema doesn’t match up with the value in the JSON file?, it will set the value of all the columns in that row to null. Instead of getting null values, you can tell Spark to fail fast'
		>> val movies8 = spark.read.option("mode","failFast").schema(badMovieSchema).json("<path>/movies.json")
	Spark will throw a RuntimeException when executing an action
		>> movies8.show(5)
			ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3) java.lang.RuntimeException: Failed to parse a value for data type BooleanType (current token: VALUE_STRING).

	'Creating DataFrames from JDBC':
		For Spark to connect to an RDBMS, it must have access to the JDBC driver JAR file at runtime. Therefore, you need to add the location of a JDBC driver to the Spark classpath.
		//Main Options for a JDBC Data Source - url, dbtable, driver
		
		>> val mysqlURL= "jdbc:mysql://localhost:3306/sakila"
		>> val filmDF = spark.read.format("jdbc")
						.option("driver", "com.mysql.jdbc.Driver")
						.option("url", mysqlURL)
						.option("dbtable", "film")
						.option("user", "<username>")
						.option("password","<password>")
						.load()

	'Creating DataFrames from .db file using JDBC and panda'
	Add additional/necessary jars if required by the following methods:

	1. /etc/spar/conf/spark-defaults.conf file setup
		spark.driver.extraClassPath = C:/tmp/vsAngular2/spark/bin/sqlite-jdbc-3.8.7.jar
		spark.executor.extraClassPath = C:/tmp/vsAngular2/spark/bin/sqlite-jdbc-3.8.7.jar

	2. If you want to add a .jar to the classpath after you have entered spark-shell, use :require. Like:
		scala> :require /path/to/file.jar
		Added '/path/to/file.jar' to classpath.

	3. While initializing spark shell itself
		./spark-shell --jars pathOfjarsWithCommaSeprated

	Using panda method:
		import sqlite3
		import pandas as pd
		db_path = '/tmp/RSM/InterpidWivi/V362.db'
		query = "SELECT name FROM sqlite_master WHERE type='table';" <-- to list the tables in that .db file
		conn = sqlite3.connect(db_path)
		a_pandas_df = pd.read_sql_query(query, conn)
		a_spark_df = sqlContext.createDataFrame(a_pandas_df)

	Using jdbc method:
		val df = spark.read.format("jdbc").options(
	  		Map(
	   		 "url" -> "jdbc:sqlite:/tmp/RSM/InterpidWivi/V362.db",
	    	 "dbtable" -> "channels;",
	   		 "driver" -> "org.sqlite.JDBC")).load()




	'Creating DataFrame from S3':
		There are no S3 libraries in the core Apache Spark project. Spark uses libraries from Hadoop to connect to S3.

		1. Specify the credentials in a configuration file, such as core-site.xml: 
			<property>
		    	<name>fs.s3a.access.key</name>
			    <value>...</value>
			</property>
			<property>
 				<name>fs.s3a.secret.key</name>
    			<value>...</value>
			</property>

		Initialize the session
			>> import org.apache.hadoop.fs.{FileSystem, Path}
			>> import org.apache.hadoop.conf.Configuration
			>> val spark = SparkSession.builder.appName("SUsingS3").getOrCreate()
 		Create an RDD from a file in S3 using the s3n protocol
			>> val s3nRdd = sc.textFile("s3n://sparkour-data/random_numbers.txt")

		2. AWS credentials inline in the S3N URI:

			>> val lyrics = sc.textFile("s3n://MyAccessKeyID:MySecretKey@bucketname/Filename.txt")

		3. Configuring AWS Credentials in SparkContext:
			>> val hadoopConf = sparkContext.hadoopConfiguration
			>> hadoopConf.set("fs.s3.impl", "org.apache.hadoop.fs.s3native.NativeS3FileSystem")
			>> sc.hadoopConfiguration.set("fs.s3n.awsAccessKeyId", s3Key)
			>> sc.hadoopConfiguration.set("fs.s3n.awsSecretAccessKey", s3Secret) // can contain "/"
			>> val myRDD = sc.textFile("s3n://myBucket/MyFilePattern")
			>> myRDD.count


`Working with Structured Operations`
....
.......
........

`Spark Use cases - PYSPARK and Spark-Scala`

'how to rename or delete a file from HDFS inside spark' 
	package com.bigdataetl
    import org.apache.hadoop.fs.{FileSystem, Path},
    import org.apache.spark.sql.SparkSession
    object Test extends App {
      val spark = SparkSession.builder.master("local[*]").appName("BigDataETL").getOrCreate()
       -- Create FileSystem object from Hadoop Configuration
      val fs = FileSystem.get(spark.sparkContext.hadoopConfiguration)
       -- Base path where Spark will produce output file
      val basePath = "/bigtadata_etl/spark/output"
      val newFileName = "renamed_spark_output"
       -- Change file name from Spark generic to new one
      fs.rename(new Path(s"$basePath/part-00000"), new Path(s"$basePath/$newFileName"))
       --Delete directories recursively using FileSystem class
	  fs.delete(new Path("/bigdata_etl/data"), true)
	  -- Delete using Scala DSL
	  s"hdfs dfs -rm -r /bigdata_etl/data/" !
	  -- Delete file
	  fs.removeAcl(new Path("/bigdata_etl/data/file_to_delete.dat"))
	  -- Delete using Scala DSL
	  s"hdfs dfs -rm /bigdata_etl/data/file_to_delete.dat" !
	  }

'How to concatenate a date to a filename in pyspark'
	import datetime
	currentdate = datetime.datetime.now().strftime("%Y-%m-%d-%H:%M:%S") 
	counts.coalesce(1).write.csv("/home/packt/Downloads/myresults3-" + currentdate + ".csv")





=============================================================================
https://spoddutur.github.io/spark-notes/distribution_of_executors_cores_and_memory_for_spark_application.html
https://stackoverflow.com/questions/37871194/how-to-tune-spark-executor-number-cores-and-executor-memory
http://site.clairvoyantsoft.com/understanding-resource-allocation-configurations-spark-application/
https://data-flair.training/forums/topic/by-default-how-many-partitions-are-created-in-rdd-in-apache-spark/
Spark examples: http://www.javachain.com/import-teradata-using-spark-into-hadoop/
Kafka
======
https://medium.com/@durgaswaroop/a-practical-introduction-to-kafka-storage-internals-d5b544f6925f

http://apachesparkbook.blogspot.com/search/label/a54%7C%20collectAsMap%28%29 --> mini basics
=============================================================================



























=============================================================================
`PIG`:

for Loading and Storing Hive Data into Pig --> https://acadgild.com/blog/loading-and-storing-hive-data-into-pig

export HCAT_HOME=/usr/local/hive/hcatalog



'Errors and Solution:'
MR job fails with below exception: 
java.net.ConnectException: Call From master/192.168.1.11 to 0.0.0.0:10020 failed on connection exception: java.net.ConnectException: Connection refused;

10020 is a port for jobHistory server, so check the port is in listen state ->
	sudo netstat -lpten |  grep -i listen
also make an entry in mapred-site.xml for the property 'mapreduce.jobhistory.address'

=============================================================================

=============================================================================
`Zookeeper`

	Read --> https://unskilledcoder.github.io/hadoop/hbase/2016/12/11/hbase-cluster-setup-with-zookeeper.html
'Installation'
	tar xzf zookeeper-3.4.6.tar.gz
	sudo mv zookeeper-3.4.6 /usr/local/zookeeper
	sudo chown -R hduser:hadoop /usr/local/zookeeper

	vi ~/.bashrc
	export ZK_HOME=/usr/local/zookeeper
	export PATH=$PATH:$ZK_HOME/bin

	cp $ZK_HOME/conf/zoo_sample.cfg $ZK_HOME/conf/zoo.cfg; vi $ZK_HOME/conf/zoo.cfg
	dataDir=/usr/local/zookeeper/data
	server.1=master:2888:3888
	server.2=dn1:2888:3888
	server.3=dn2:2888:3888

	in dn1 -> sudo mkdir /usr/local/zookeeper; sudo chown -R hduser:hadoop /usr/local/zookeeper
	in dn2 -> sudo mkdir /usr/local/zookeeper; sudo chown -R hduser:hadoop /usr/local/zookeeper

	scp -r /usr/local/zookeeper/\* hduser@dn1:/usr/local/zookeeper
	scp -r /usr/local/zookeeper/\* hduser@dn2:/usr/local/zookeeper

	if you have two servers (SERVER1 and SERVER2) for which you have created "myid" files in dataDir for zookeeper as below

	SERVER1 (myid)
	1

	SERVER2 (myid)
	2

	ssh master mkdir $ZK_HOME/data
	ssh dn1 mkdir $ZK_HOME/data
	ssh dn2 mkdir $ZK_HOME/data

	lsof -i -P | grep 2181
	sudo netstat -nlpo | grep :2181 

	${HBASE_HOME}/bin/hbase-daemons.sh {start,stop} zookeeper

	cd /usr/local/zookeeper/bin/; ./zkServer.sh stop
	cd /usr/local/zookeeper/bin/; ./zkServer.sh start

	sudo /etc/init.d/hbase-master restart
	sudo /etc/init.d/hbase-regionserver restart
	sudo /etc/init.d/zookeeper-server status

=============================================================================
`HBASE`
'Installation'
	tar xzf hbase-0.98.4-hadoop2-bin.tar.gz
	sudo mv hbase-0.98.4-hadoop2 /usr/local/hbase
	sudo chown -R hduser:hadoop /usr/local/hbase

	cd /usr/local/hbase/conf
	echo "export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_181" >> hbase-env.sh
	echo "export HBASE_MANAGES_ZK=false" >> hbase-env.sh

	vi ~/.bashrc
	export HBASE_HOME=/usr/local/hbase
	export PATH=$PATH:$HBASE_HOME/bin
	export HBASE_MANAGES_ZK=false

	hadoop fs -mkdir /user/hduser/hbase

	vi /usr/local/hbase/conf/hbase-site.xml
	<configuration>
	<property>
		<name>hbase.cluster.distributed</name>
		<value>true</value>
	</property>
	<property>
		<name>hbase.rootdir</name>
		<value>hdfs://master:54310/user/hduser/hbase</value>
	</property>
	<property>
		<name>hbase.zookeeper.property.dataDir</name>
		<value>/usr/local/zookeeper/data</value>
	</property>
	<property>
	    <name>hbase.master</name>
	    <value>master:60000</value>
	</property>
	<property>
	    <name>hbase.master.port</name>
	    <value>60000</value>
	    <description>The port master should bind to.</description>
	</property>
	<!-- zookeeper cluster we setup in previous post -->
	<property>
	    <name>hbase.zookeeper.quorum</name>
	    <value>master,dn1,dn2</value>
	</property>
	<property>
		<name>hbase.zookeeper.property.clientPort</name>
		<value>2181</value>
	</property>
	<!-- 2 since we have 2 slaves for data -->
	<property>
	    <name>dfs.replication</name>
	    <value>2</value>
	</property>
	</configuration>

	. ~/.bashrc
	Write correct region server hostnames into $HBASE_HOME/conf/regionservers
	echo dn1 > $HBASE_HOME/conf/regionservers
	echo dn2 >> $HBASE_HOME/conf/regionservers

	Copy the environment variables and HBase config to other nodes
	scp -r ~/.bashrc hduser@dn1:/home/hduser
	scp -r ~/.bashrc hduser@dn2:/home/hduser

	sudo mkdir /usr/local/hbase/  && sudo chown -R hduser:hadoop /usr/local/hbase --> in dn1 and dn2

	scp -r /usr/local/hbase/\* hduser@dn1:/usr/local/hbase
	scp -r /usr/local/hbase/\* hduser@dn2:/usr/local/hbase

start-hbase.sh
	>list --> to list tables
	>create 'Patient1','Personal','Medical'
	>put 'Patient','001','Personal:pname','Ramesh'

	>alter 'Patient1',{NAME=>'Personal',VERSIONS=>3} --> to change the update history from 1 to 3
	>put 'Patient1','001','Personal:pname','Ramesh k'
	>put 'Patient1','001','Personal:pname','k Ramesh'
	>scan 'Patient1',{VERSIONS => 3} --> to list the values of table with last 3 update history

	>delete 'Patient1','002','Personal:pname' --> delete a specific column from rowkey
	>deleteall 'Patient1','001' --> delete entire rowkey details
	>alter 'custmaster',{NAME=>'insurancehive',METHOD=>'delete'}

`Error and SOlution`
1. The following error appears in /usr/local/hbase/logs/hbase-hduser-master-master.log

	2018-10-04 12:07:46,986 FATAL [master:master:60000] master.HMaster: Master server abort: loaded coprocessors are: []
	2018-10-04 12:07:46,987 FATAL [master:master:60000] master.HMaster: Unhandled exception. Starting shutdown.
	org.apache.hadoop.hbase.TableExistsException: hbase:namespace
	        at org.apache.hadoop.hbase.master.handler.CreateTableHandler.prepare(CreateTableHandler.java:120)
	        at org.apache.hadoop.hbase.master.TableNamespaceManager.createNamespaceTable(TableNamespaceManager.java:232)
	        at org.apache.hadoop.hbase.master.TableNamespaceManager.start(TableNamespaceManager.java:86)
	        at org.apache.hadoop.hbase.master.HMaster.initNamespace(HMaster.java:1051)
	        at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:914)
	        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:603)
	        at java.lang.Thread.run(Thread.java:748)
	2018-10-04 12:07:46,989 INFO  [master:master:60000] master.HMaster: Aborting
	.
	.
	.
	2018-10-04 12:07:47,122 INFO  [master:master:60000] master.HMaster: HMaster main thread exiting
	2018-10-04 12:07:47,123 ERROR [main] master.HMasterCommandLine: Master exiting
	java.lang.RuntimeException: HMaster Aborted
	        at org.apache.hadoop.hbase.master.HMasterCommandLine.startMaster(HMasterCommandLine.java:194)
	        at org.apache.hadoop.hbase.master.HMasterCommandLine.run(HMasterCommandLine.java:135)
	        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	        at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)
	        at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:2793)
	Thu Oct  4 12:10:15 IST 2018 Stopping hbase (via master)

	According the error above, there should be a table named hbase:namespace for maintaining the information of namespace tables. The above error is displayed when the HMaster creates the namespace directory under /hbase-unsecure directory(for non-secured cluster) and /hbase-secure(for secured cluster) while starting the process.

	Manually repair the Hbase Metastore by
	$HBASE_HOME/bin/hbase org.apache.hadoop.hbase.util.hbck.OfflineMetaRepair
	$ZK_HOME/bin/zkCli.sh
	-> ls /
	-> rmr /hbase-unsecure
	-> quit
	Restart HBase Service.

2. 'Error creating table in HBASE'. previous tables were available and error appeared after fresh start. Last session of Hbase closed properly but not zookeeper (power shut down)

hbase(main):002:0> create 'custmaster', 'customer'

ERROR: java.io.IOException: Table Namespace Manager not ready yet, try again later
        at org.apache.hadoop.hbase.master.HMaster.getNamespaceDescriptor(HMaster.java:3179)
        at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:1735)
        at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:1774)
        at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:40470)
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2027)
        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:98)
        at org.apache.hadoop.hbase.ipc.FifoRpcScheduler$1.run(FifoRpcScheduler.java:74)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Work Around:  Running hbck to identify inconsistencies
[hduser@master ~]$ hbase hbck
				 >Version: 0.98.4-hadoop2
				 >Number of live region servers: 2
				 >Number of dead region servers: 0
				 >Master: master,60000,1538793456542
				 >Number of backup masters: 0
				 >Average load: 0.0
				 >Number of requests: 11
				 >Number of regions: 0
				 >Number of regions in transition: 1
				 >
				 >ERROR: META region or some of its attributes are null.
				 >ERROR: hbase:meta is not found on any region.
				 >ERROR: hbase:meta table is not consistent. Run HBCK with proper fix options to fix hbase:meta inconsistency. Exiting...
				 .
				 .
				 .
				 >Summary:				 >
				 >3 inconsistencies detected.
				 >Status: INCONSISTENT

[hduser@master ~]$ hbase hbck -details
				 .
				 .
				 >ERROR: META region or some of its attributes are null.
				 >ERROR: hbase:meta is not found on any region.
				 >ERROR: hbase:meta table is not consistent. Run HBCK with proper fix options to fix hbase:meta inconsistency. Exiting...
				 >Summary:
				 >3 inconsistencies detected.
				 >Status: INCONSISTENT

Meta store corrupted because the underlying file/blocks in HDFS corrupted. Either manually repair the Hbase Metastore by
	>$HBASE_HOME/bin/hbase org.apache.hadoop.hbase.util.hbck.OfflineMetaRepair
	>zkCli.sh
	>[zk: localhost:2181(CONNECTED) 0] ls /
	>[zookeeper, hbase]
	>[zk: localhost:2181(CONNECTED) 1] rmr hbase /
	>Command failed: java.lang.IllegalArgumentException: Path must start with / character
	>[zk: localhost:2181(CONNECTED) 2] rmr /hbase /
	>[zk: localhost:2181(CONNECTED) 3] ls /
	>[zookeeper]
	>[zk: localhost:2181(CONNECTED) 4] quit

	hadoop fs -rm -r /user/hduser/hbase
(OR)

Try to run hadoop fsck / to find out the corrupted files and repair 

[hduser@master ~]$ hadoop fsck /
	DEPRECATED: Use of this script to execute hdfs command is deprecated.
	Instead use the hdfs command for it.

	18/10/06 09:52:00 WARN util.NativeCodeLoader: Unable to load native-hadoop libr
	ary for your platform... using builtin-java classes where applicable
	Connecting to namenode via http://master:50070/fsck?ugi=hduser&path=%2F
	FSCK started by hduser (auth:SIMPLE) from /192.168.1.11 for path / at Sat Oct 0
	6 09:52:02 IST 2018
	...............................................................................
	..
	/user/hduser/hbase/.hbck/hbase-1538798774320/data/hbase/meta/1588230740/info/35
	9783d4cd07419598264506bac92dcf: CORRUPT blockpool BP-1664228054-192.168.1.11-15
	35828595216 block blk_1073744002

	/user/hduser/hbase/.hbck/hbase-1538798774320/data/hbase/meta/1588230740/info/35                                                   9783d4cd07419598264506bac92dcf: MISSING 1 blocks of total size 3934 B.........
	/user/hduser/hbase/data/default/IDX_STOCK_SYMBOL/a27db76f84487a05f3e1b8b74c13fa
	78/0/c595bf49443f4daf952df6cdaad79181: CORRUPT blockpool BP-1664228054-192.168.
	1.11-1535828595216 block blk_1073744000

	/user/hduser/hbase/data/default/IDX_STOCK_SYMBOL/a27db76f84487a05f3e1b8b74c13fa
	78/0/c595bf49443f4daf952df6cdaad79181: MISSING 1 blocks of total size 1354 B...
	.........
	...
	/user/hduser/hbase/data/default/SYSTEM.CATALOG/d63574fdd00e8bf3882fcb6bd53c3d83
	/0/dcb68bbb5e394d19b06db7f298810de0: CORRUPT blockpool BP-1664228054-192.168.1.
	11-1535828595216 block blk_1073744001

	/user/hduser/hbase/data/default/SYSTEM.CATALOG/d63574fdd00e8bf3882fcb6bd53c3d83
	/0/dcb68bbb5e394d19b06db7f298810de0: MISSING 1 blocks of total size 2283 B.....                                                   ......................Status: CORRUPT
	 Total size:    4232998 B
	 Total dirs:    109
	 Total files:   129
	 Total symlinks:                0
	 Total blocks (validated):      125 (avg. block size 33863 B)
	  ********************************
	  UNDER MIN REPLICAED BLOCKS:      3 (2.4 %)
	  dfs.namenode.replication.min: 1
	  CORRUPT FILES:        3
	  MISSING BLOCKS:       3
	  MISSING SIZE:         7571 B
	  CORRUPT BLOCKS:       3
	  ********************************
	 Minimally replicated blocks:   122 (97.6 %)
	 Over-replicated blocks:        0 (0.0 %)
	 Under-replicated blocks:       0 (0.0 %)
	 Mis-replicated blocks:         0 (0.0 %)
	 Default replication factor:    2
	 Average block replication:     1.952
	 Corrupt blocks:                3
	 Missing replicas:              0 (0.0 %)
	 Number of data-nodes:          2
	 Number of racks:               1
	FSCK ended at Sat Oct 06 09:52:02 IST 2018 in 66 milliseconds


	The filesystem under path '/' is CORRUPT

bin/hadoop fsck / -delete
				
==================================================================================
`Phoenix`
'Installation'
	Read https://dzone.com/articles/apache-phoenix-sql-driver

	vi /usr/local/hbase/conf/hbase-site.xml

	<property>
	<name>hbase.regionserver.wal.codec</name>
	<value>org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec</value>
	</property>

	.bashrc
	export PHOENIX_HOME=/usr/local/phoenix
	export PATH=$PATH:$PHOENIX_HOME/bin

	cd /home/hduser/install
	tar xzf phoenix-4.6.0-HBase-0.98-bin.tar.gz
	sudo mv phoenix-4.6.0-HBase-0.98-bin /usr/local/phoenix
	sudo chown -R hduser:hadoop /usr/local/phoenix
	Add below Jar files to hbase lib folder
	cd /usr/local/phoenix/

	Copy the below jars into region servers (dn1, dn2) path /usr/local/hbase/lib/

	cp phoenix-4.6.0-HBase-0.98-client-minimal.jar /usr/local/hbase/lib/
	cp phoenix-core-4.6.0-HBase-0.98.jar /usr/local/hbase/lib/ 
